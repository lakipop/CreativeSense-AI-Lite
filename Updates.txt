
///////////Phase 1//////////////////

Project Goal:
Create a "Multimodal AI Dashboard" application.

Core Technology Stack:
- Framework: React (using Vite)
- Styling: Tailwind CSS
- Language: JavaScript (ES6+)

Core App Structure:
- Create a modular, component-based React application.
- `App.jsx`: The main layout component. It should have a clean, modern dashboard UI.
- `src/components/`: Create a directory for our three core features.
- `src/utils/geminiApi.js`: Create a utility file to manage the Gemini API client and all API call functions. It should be configured to get the API key from a `.env` file.

Detailed Scaffolding Request:

1.  **Main Layout (`App.jsx`):**
    * Create a modern, responsive dashboard layout using Tailwind CSS.
    * The layout should be a 3-panel grid (or a tabbed interface) where each of the three features below can be displayed and operated.
    * The user must be able to view and use all three features concurrently.

2.  **Feature 1: Live Screen Understanding (Component Stub)**
    * Create a new component: `src/components/ScreenUnderstander.jsx`.
    * In this component, create the basic UI:
        * A "Start Screen Share" button.
        * A `<video>` element (muted, autoplay) to display the screen stream.
        * An output `<div>` to display the AI's description.
    * Stub out the core function: `const handleStartShare = () => { /* Placeholder: Add getDisplayMedia logic here */ }`.
    * Stub out the API call loop: `const startDescriptionLoop = () => { /* Placeholder: Add snapshot and Gemini Vision call logic here */ }`.
    * Import and render this component in `App.jsx`.

3.  **Feature 2: Conversational Voice (Component Stub)**
    * Create a new component: `src/components/VoiceChat.jsx`.
    * In this component, create the basic UI:
        * A "Start Listening" (microphone) button.
        * A `<div>` to show the conversation transcript (chat log).
    * Stub out the core functions:
        * `const handleToggleListening = () => { /* Placeholder: Add SpeechRecognition logic here */ }`.
        * `const sendTextToGemini = (text) => { /* Placeholder: Add Gemini text chat API call logic here */ }`.
        * `const speakResponse = (text) => { /* Placeholder: Add SpeechSynthesis logic here */ }`.
    * Import and render this component in `App.jsx`.

4.  **Feature 3: RAG-Powered Chat (Component Stub)**
    * Create a new component: `src/components/RagChat.jsx`.
    * In this component, create the basic UI:
        * An `<input type="file" multiple>` for uploading documents (.txt, .md).
        * A `<div>` to show the chat interface (message log and text input).
    * Stub out the core functions:
        * `const handleFileUpload = (event) => { /* Placeholder: Add file reading, chunking, and embedding logic here */ }`.
        * `const handleRagChatSubmit = (query) => { /* Placeholder: Add query embedding, vector search, and grounded prompt logic here */ }`.
    * Import and render this component in `App.jsx`.

5.  **Gemini Connectivity (`src/utils/geminiApi.js`):**
    * Set up the GoogleGenerativeAI client.
    * Stub out placeholder functions:
        * `getGeminiVisionResponse(base64Image, prompt)`
        * `getGeminiChatResponse(prompt)`
        * `getGeminiEmbedding(text)`
    * Ensure this file correctly reads the `VITE_GEMINI_API_KEY` from a `.env` file.


    ////////////Plase2////////////


Project Context:
We have a React + Vite + Tailwind CSS project generated by Google AI Studio. The file structure is:
- `App.jsx` (main layout)
- `src/components/ScreenUnderstander.jsx`
- `src/components/VoiceChat.jsx`
- `src/components/RagChat.jsx`
- `src/utils/geminiApi.js` (with a configured Gemini client)

All components are stubbed out. Our task is to fully implement all features, refactor for professional-grade state and backend logic, and polish the UI.

Your Role: You are my pair programmer (Sonnet 4.5 Pro). We will implement this feature by feature.

---

**Task 1: Global State Management**
- Integrate `Zustand` (or React Context) for global state.
- We need to manage:
    - `isScreenSharingActive` (boolean)
    - `isVoiceChatActive` (boolean)
    - `isRagChatActive` (boolean)
    - `chatHistory` (array)
    - `documentEmbeddings` (array)

---

**Task 2: Full Implementation of `ScreenUnderstander.jsx`**
1.  Implement `handleStartShare`:
    - Use `navigator.mediaDevices.getDisplayMedia({ video: true })`.
    - Pipe the resulting `MediaStream` to the `<video>` element.
    - On success, call `startDescriptionLoop()`.
2.  Implement `startDescriptionLoop`:
    - Use `setInterval` (e.g., every 3 seconds).
    - Inside the loop, snapshot the `<video>` to a hidden `<canvas>`, get a `base64` data URL.
    - Call the `getGeminiVisionResponse` function from `geminiApi.js` with the image and a prompt like "Describe this frame."
    - Stream the text response to the UI.
    - Handle stopping the loop when the stream ends.

---

**Task 3: Full Implementation of `VoiceChat.jsx`**
1.  Implement `handleToggleListening`:
    - Use the Web Speech API (`SpeechRecognition`).
    - Handle state changes (idle, listening, processing).
    - On `onresult`, get the final transcript.
    - Call `sendTextToGemini(transcript)`.
2.  Implement `sendTextToGemini`:
    - Add the user's transcript to the `chatHistory` state.
    - Call the `getGeminiChatResponse` function.
    - On response, add the AI's response to `chatHistory`.
    - Call `speakResponse(aiText)`.
3.  Implement `speakResponse`:
    - Use `SpeechSynthesis` to speak the text.
4.  UI: Render the `chatHistory` state into a scrolling chat bubble interface.

---

**Task 4: Initial Implementation of `RagChat.jsx` (Client-Side)**
1.  Implement `handleFileUpload`:
    - Read files (`.txt`, `.md`) using `FileReader`.
    - Implement a `chunkText` utility function (e.g., 500-char chunks, 50-char overlap).
    - For each chunk, call `getGeminiEmbedding` from `geminiApi.js`.
    - Store the results `{ text: "...", embedding: [...] }` in the `documentEmbeddings` global state.
2.  Implement `handleRagChatSubmit`:
    - Get the user's `query`.
    - Call `getGeminiEmbedding(query)` to get the query vector.
    - Implement a `cosineSimilarity` utility function.
    - Perform a client-side search: compare the query vector against all vectors in `documentEmbeddings`.
    - Get the Top 3 text chunks.
    - Construct a grounded prompt:
      ```
      "Context:
      [Chunk 1]
      [Chunk 2]
      [Chunk 3]
      Based only on this context, answer the query: [Query]"
      ```
    - Send this prompt to `getGeminiChatResponse` and display the answer.

---

**Task 5: Professional Refactor (Move RAG to Backend)**
The client-side RAG is a good prototype, but it's slow and insecure. Let's create a proper backend.
1.  **Create Backend:**
    - Create a `backend/` directory with a `Node.js + Express` server.
    - Add a `package.json` and install `express`, `cors`, `multer`, `pdf-parse`, and `@google/generative-ai`.
    - Secure the Gemini API key here using `dotenv`.
2.  **Create Endpoints:**
    - **`POST /api/upload`**:
        - Use `multer` for file uploads (support .txt, .md, .pdf).
        - Use `pdf-parse` for PDFs.
        - Perform all chunking and embedding *on the server*.
        - Store embeddings in a server-side vector store (for this project, a simple in-memory store or a JSON file is fine).
    - **`POST /api/rag-chat`**:
        - Receives a `query` in the body.
        - Embeds the query.
        - Performs the vector search *on the server*.
        - Constructs the grounded prompt and calls the Gemini API *from the server*.
        - Returns the final, grounded answer to the client.
3.  **Refactor Frontend (`RagChat.jsx`):**
    - Remove all embedding and search logic from the component.
    - `handleFileUpload` now just `fetch`-posts the files to `http://localhost:3001/api/upload`.
    - `handleRagChatSubmit` now just `fetch`-posts the query to `http://localhost:3001/api/rag-chat` and displays the response.

---

**Task 6: Final UI/UX Polish**
- Review `App.jsx`. Use Tailwind CSS to make the 3-panel layout clean, responsive, and professional.
- Add loading spinners and error-handling messages for all API calls in all three components.
- Ensure the app is fully responsive on mobile.